"""
HOW:   
  Run this script to ingest audio, transcribe it, and optionally run various diarization/embedding workflows.
  You can also use it to download videos from various providers supported by yt-dlp.

  [IMPORTANT: Directory Context]
  You MUST run this script from the `apps/speaker-diarization-benchmark` directory to ensure all dependencies 
  (especially those managed by `uv`) are correctly resolved.
  
  Correct usage:
    cd apps/speaker-diarization-benchmark
    uv run audio_ingestion.py <command> ...

  [Commands]
  1. Transcribe audio:
     uv run audio_ingestion.py transcribe <audio_path> --start-time 0 --end-time 60

  2. Run diarization:
     uv run audio_ingestion.py diarize <audio_path> --workflow pyannote

  3. Identify speakers:
     uv run audio_ingestion.py identify --video-id <uuid> --threshold 0.5 --execute

  4. Full pipeline (download ‚Üí transcribe ‚Üí diarize ‚Üí identify):
     uv run audio_ingestion.py ingest <URL_or_path> --start-time 0 --end-time 240

  5. Download a video:
     uv run audio_ingestion.py download <URL> --output-dir data/clips

  [Inputs (Common)]
  - --start-time: Start time in seconds (default: 0)
  - --end-time: End time in seconds (default: full file)
  - --dry-run: Show what would happen without executing
  - --verbose: Enable verbose logging
  
  [Outputs]
  - Saves results to InstantDB (via TypeScript server)
  - Caches transcriptions in data/cache/transcriptions

WHO:
  Antigravity, Claude AI
  (Context: Audio Ingestion System)

WHAT:
  Main entry point for the audio ingestion system.
  It orchestrates:
  1. Audio/Video downloading via `yt-dlp`
  2. Transcription via `mlx_whisper` (cached)
  3. Diarization/Embedding via configured workflows
  4. Speaker identification via KNN search
  5. Saving to InstantDB

WHEN:
  2025-12-05
  Last Modified: 2025-12-09
  Change Log:
  - 2025-12-09: Added --segment-source whisper for synthetic diarization segments
  - 2025-12-08: Added transcribe, identify, ingest commands
  - 2025-12-08: Added service health checks
  - 2025-12-08: Removed manifest.json support (InstantDB only)
  
WHERE:
  apps/speaker-diarization-benchmark/audio_ingestion.py

WHY:
  To provide a unified CLI for bringing audio data into the benchmarking system and running experiments.
  InstantDB is now the single source of truth - no more manifest.json.
"""

import os
import sys
import time
import logging
import json
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add parent directory to sys.path for imports
sys.path.append(str(Path(__file__).parent))

from ingestion.args import parse_args
from ingestion.config import (
    TranscribeConfig,
    DiarizeConfig,
    IdentifyConfig,
    IngestConfig,
    IngestionConfig,
    DownloadConfig,
    ServerConfig,
)
from ingestion.download import download_video
from ingestion.ground_truth_server import run_server
from ingestion.health import check_services
from ingestion.dry_run import print_dry_run_output
from ingestion.metrics import RunMetrics, track_run
from transcribe import transcribe, TranscriptionResult, Segment, Word
from utils import get_git_info

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def run_transcribe(config: TranscribeConfig) -> None:
    """Run transcription command."""
    if config.dry_run:
        print_dry_run_output(config)
        return
    
    logger.info(f"Transcribing: {config.audio_path}")
    logger.info(f"Time range: {config.start_time}s - {config.end_time or 'end'}s")
    
    result = transcribe(
        str(config.audio_path),
        start_time=config.start_time if config.start_time > 0 else None,
        end_time=config.end_time,
    )
    
    # Output as JSON
    print(json.dumps(result.model_dump(), indent=2))


def run_diarize(config: DiarizeConfig) -> None:
    """Run diarization command."""
    # Check services
    check_services(require_instant_server=True, require_postgres=True)
    
    if config.dry_run:
        print_dry_run_output(config)
        return
    
    logger.info(f"Diarizing: {config.audio_path}")
    logger.info(f"Workflow: {config.workflow.name}")
    logger.info(f"Time range: {config.start_time}s - {config.end_time or 'end'}s")
    
    # Get workflow
    from ingestion.args import get_workflow
    workflow = get_workflow(config.workflow)
    
    # Run transcription first (needed for some workflows)
    transcription_result = _get_or_create_transcription(
        config.audio_path,
        config.start_time,
        config.end_time,
    )
    
    # Run diarization
    segments, stats = workflow.run(config.audio_path, transcription_result)
    
    # Output results
    print(json.dumps({
        "segments": segments,
        "stats": stats,
    }, indent=2))


def run_identify(config: IdentifyConfig) -> None:
    """Run speaker identification command."""
    # Check services
    check_services(require_instant_server=True, require_postgres=True)
    
    if not config.execute:
        print_dry_run_output(config)
        return
    
    from ingestion.identify import identify_speakers, print_plan, execute_plan
    from ingestion.instant_client import InstantClient
    from src.embeddings.pgvector_client import PgVectorClient
    
    logger.info(f"Identifying speakers for video: {config.video_id[:8]}...")
    
    # Initialize clients
    instant_client = InstantClient()
    pg_dsn = os.getenv("SPEAKER_DB_DSN") or os.getenv("POSTGRES_DSN") or "postgresql://postgres:postgres@localhost:5433/postgres"
    pg_client = PgVectorClient(pg_dsn)
    
    # Get audio path if not provided
    audio_path = str(config.audio_path) if config.audio_path else None
    if not audio_path:
        video = instant_client.get_video(config.video_id)
        audio_path = video.get("filepath")
    
    # Run identification
    plan = identify_speakers(
        instant_client=instant_client,
        pg_client=pg_client,
        video_id=config.video_id,
        start_time=config.start_time if config.start_time > 0 else None,
        end_time=config.end_time,
        threshold=config.threshold,
        top_k=config.top_k,
        audio_path=audio_path,
    )
    
    # Print results
    print_plan(plan)
    
    # Execute if requested
    if config.execute:
        execute_plan(instant_client, pg_client, plan)


def run_ingest(config: IngestConfig) -> None:
    """
    Run full ingestion pipeline with caching.
    
    Modes:
      --dry-run: Show plan only (no compute)
      --preview: Run compute, show preview, save to markdown, don't save to DB
      (default): Run compute, show preview, ask confirmation, save to DB
      --yes: Run compute, skip confirmation, save to DB
    """
    from ingestion.cache import (
        extract_video_id,
        TranscriptionCache,
        DiarizationCache,
        IdentificationCache,
        PreviewCache,
        get_embedding_count,
    )
    from ingestion.preview import generate_preview_markdown, save_preview, print_preview_summary
    
    # Check services
    check_services(require_instant_server=True, require_postgres=True)
    
    # Dry run = show plan only (no compute)
    if config.dry_run:
        print_dry_run_output(config)
        return
    
    logger.info("Starting full ingestion pipeline")
    start_time_global = time.time()
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 1: Download (if URL)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    video_id = extract_video_id(config.source)
    
    if config.is_url:
        logger.info(f"Step 1: Downloading from {config.source}")
        expected_path = config.output_dir / f"{video_id}.wav"
        
        if expected_path.exists():
            logger.info(f"   ‚úÖ Cache HIT: {expected_path}")
            audio_path = expected_path
        else:
            download_config = DownloadConfig(
                url=config.source,
                output_dir=config.output_dir,
                verbose=config.verbose,
            )
            audio_path = download_video(download_config)
            if not audio_path:
                logger.error("Download failed")
                return
            audio_path = Path(audio_path)
    else:
        audio_path = Path(config.source)
        video_id = audio_path.stem
        logger.info(f"Step 1: Using local file {audio_path}")
    
    if not audio_path.exists():
        logger.error(f"Audio file not found: {audio_path}")
        return
    
    # Determine the end time for caching
    # If end_time is None, we need to compute full file (use a large number)
    cache_end_time = config.end_time or 99999.0
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 2: Transcribe (with caching)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    logger.info(f"Step 2: Transcribing {audio_path}")
    
    trans_cache = TranscriptionCache(
        video_id=video_id,
        tool="mlx-whisper",
        model="whisper-large-v3-turbo",
    )
    
    # Log cache key and location for debugging
    logger.info(f"   üìÅ Cache key: {trans_cache.cache_key}")
    logger.info(f"   üìÅ Cache file: {trans_cache.cache_path}")
    
    # Initialize metrics tracking for transcription
    transcription_metrics = RunMetrics(input_duration_seconds=cache_end_time)
    
    if trans_cache.has_range(cache_end_time):
        logger.info(f"   ‚úÖ Cache HIT: transcription [0-{cache_end_time}s]")
        logger.info(f"   ‚Ü≥ Loading from: {trans_cache.cache_path.name}")
        transcription_data = trans_cache.get_filtered(config.start_time, cache_end_time)
        # Reconstruct TranscriptionResult
        transcription_result = _dict_to_transcription_result(transcription_data)
        logger.info(f"   ‚Ü≥ Loaded {len(transcription_result.segments)} segments, {sum(len(s.words) for s in transcription_result.segments)} words from cache")
        # Load cached metrics if available
        cached_metadata = trans_cache.get_metadata()
        if cached_metadata and cached_metadata.get("metrics"):
            m = cached_metadata["metrics"]
            transcription_metrics.processing_time_seconds = m.get("processing_time_seconds")
            transcription_metrics.peak_memory_mb = m.get("peak_memory_mb")
            transcription_metrics.cost_usd = m.get("cost_usd")
    else:
        cached_end = trans_cache.get_cached_end()
        if cached_end:
            logger.info(f"   ‚ö†Ô∏è Cache MISS: have [0-{cached_end}s], need [0-{cache_end_time}s]")
            logger.info(f"   ‚Ü≥ Cache exists but doesn't cover requested range")
            logger.info(f"   ‚Ü≥ Will recompute full range [0-{cache_end_time}s]")
        else:
            logger.info(f"   ‚ö†Ô∏è Cache MISS: no cache file found")
            logger.info(f"   ‚Ü≥ Will compute transcription [0-{cache_end_time}s]")
        
        # Compute transcription with metrics tracking
        logger.info(f"   üîÑ Running MLX Whisper transcription...")
        with track_run(input_duration_seconds=cache_end_time) as transcription_metrics:
            transcription_result = transcribe(
                str(audio_path),
                start_time=None,  # Always start from 0 for caching
                end_time=config.end_time,
            )
        
        logger.info(f"   ‚è±Ô∏è Transcription completed in {transcription_metrics.processing_time_seconds:.2f}s")
        if transcription_metrics.peak_memory_mb:
            logger.info(f"   üíæ Peak memory: {transcription_metrics.peak_memory_mb:.1f}MB")
        if transcription_metrics.realtime_factor:
            logger.info(f"   ‚ö° {transcription_metrics.realtime_factor:.1f}x realtime")
        
        # Save to cache with metrics
        trans_cache.save(
            result=transcription_result.model_dump() if hasattr(transcription_result, 'model_dump') else transcription_result.dict(),
            end_time=cache_end_time,
            metrics=transcription_metrics.to_dict(),
        )
        logger.info(f"   üíæ Saved transcription cache: {trans_cache.cache_path.name}")
    
    word_count = sum(len(s.words) for s in transcription_result.segments)
    logger.info(f"   üìä Transcription result: {len(transcription_result.segments)} segments, {word_count} words")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 3: Diarize (with caching) OR Create Synthetic Segments
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Determine the workflow name based on segment source
    if config.segment_source == "whisper":
        workflow_name = "whisper_identified"
        logger.info(f"Step 3: Creating synthetic diarization segments from Whisper transcription")
    else:
        workflow_name = config.workflow
        logger.info(f"Step 3: Running diarization (workflow: {workflow_name})")
    
    diar_cache = DiarizationCache(
        video_id=video_id,
        workflow=workflow_name,
    )
    
    # Log cache key and location for debugging
    logger.info(f"   üìÅ Cache key: {diar_cache.cache_key}")
    logger.info(f"   üìÅ Cache file: {diar_cache.cache_path}")
    
    # Initialize metrics tracking for diarization
    diarization_metrics = RunMetrics(input_duration_seconds=cache_end_time)
    
    if diar_cache.has_range(cache_end_time):
        logger.info(f"   ‚úÖ Cache HIT: diarization [0-{cache_end_time}s]")
        logger.info(f"   ‚Ü≥ Loading from: {diar_cache.cache_path.name}")
        segments = diar_cache.get_filtered(config.start_time, cache_end_time)
        stats = diar_cache.get_stats() or {}
        speaker_labels = set(s.get("speaker", "UNKNOWN") for s in segments)
        logger.info(f"   ‚Ü≥ Loaded {len(segments)} segments, {len(speaker_labels)} speakers from cache")
        # Load cached metrics if available
        cached_metadata = diar_cache.get_metadata()
        if cached_metadata and cached_metadata.get("metrics"):
            m = cached_metadata["metrics"]
            diarization_metrics.processing_time_seconds = m.get("processing_time_seconds")
            diarization_metrics.peak_memory_mb = m.get("peak_memory_mb")
            diarization_metrics.cost_usd = m.get("cost_usd")
    else:
        cached_end = diar_cache.get_cached_end()
        if cached_end:
            logger.info(f"   ‚ö†Ô∏è Cache MISS: have [0-{cached_end}s], need [0-{cache_end_time}s]")
            logger.info(f"   ‚Ü≥ Cache exists but doesn't cover requested range")
            logger.info(f"   ‚Ü≥ Will recompute full range [0-{cache_end_time}s]")
        else:
            logger.info(f"   ‚ö†Ô∏è Cache MISS: no cache file found")
            logger.info(f"   ‚Ü≥ Will compute diarization [0-{cache_end_time}s]")
        
        # Branch based on segment source
        if config.segment_source == "whisper":
            # Create synthetic diarization segments from Whisper transcription
            logger.info(f"   üîÑ Creating synthetic segments from Whisper transcription...")
            with track_run(input_duration_seconds=cache_end_time) as diarization_metrics:
                segments, stats = _create_synthetic_diarization_segments(transcription_result, audio_path)
            
            logger.info(f"   ‚è±Ô∏è Synthetic segments created in {diarization_metrics.processing_time_seconds:.2f}s")
        else:
            # Run PyAnnote diarization
            # Slice audio to requested time range for faster diarization
            # This is critical - full audio file can be 767MB (1+ hr), but we only need 60s
            from ingestion.audio_utils import slice_audio
            
            original_size = audio_path.stat().st_size / (1024 * 1024)
            logger.info(f"   üî™ Slicing audio ({original_size:.1f}MB) to [0-{cache_end_time}s]...")
            
            sliced_audio = slice_audio(
                audio_path=str(audio_path),
                start_time=0,  # Always start from 0 for cache consistency
                end_time=cache_end_time,
            )
            sliced_size = sliced_audio.stat().st_size / (1024 * 1024)
            logger.info(f"   ‚Ü≥ Sliced: {sliced_audio.name} ({sliced_size:.1f}MB)")
            
            # Compute diarization on sliced audio with metrics tracking
            from ingestion.args import get_workflow
            from ingestion.config import WorkflowConfig
            
            workflow_config = WorkflowConfig(name=config.workflow)
            workflow = get_workflow(workflow_config)
            
            logger.info(f"   üîÑ Running PyAnnote diarization on sliced audio...")
            with track_run(input_duration_seconds=cache_end_time) as diarization_metrics:
                segments, stats = workflow.run(sliced_audio, transcription_result)
            
            logger.info(f"   ‚è±Ô∏è Diarization completed in {diarization_metrics.processing_time_seconds:.2f}s")
            if diarization_metrics.peak_memory_mb:
                logger.info(f"   üíæ Peak memory: {diarization_metrics.peak_memory_mb:.1f}MB")
            if diarization_metrics.realtime_factor:
                logger.info(f"   ‚ö° {diarization_metrics.realtime_factor:.1f}x realtime")
        
        # Save to cache with metrics
        diar_cache.save(
            segments=segments,
            stats=stats,
            end_time=cache_end_time,
            metrics=diarization_metrics.to_dict(),
        )
        logger.info(f"   üíæ Saved diarization cache: {diar_cache.cache_path.name}")
    
    speaker_labels = set(s.get("speaker", "UNKNOWN") for s in segments)
    logger.info(f"   üìä Diarization result: {len(segments)} segments, {len(speaker_labels)} speakers ({', '.join(sorted(speaker_labels))})")
    
    # Prepare video data
    video_data = {
        "id": video_id,
        "title": config.title or audio_path.stem,
        "filepath": str(audio_path.resolve()),
        "source_url": config.source if config.is_url else None,
    }
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # STEP 4: Identify (with caching)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    identification_plan = None
    pg_client = None
    instant_client = None
    
    if not config.skip_identify:
        logger.info("Step 4: Running speaker identification")
        
        try:
            from src.embeddings.pgvector_client import PgVectorClient
            from ingestion.identify import identify_speakers
            from ingestion.instant_client import InstantClient
            
            instant_client = InstantClient()
            # Note: Use SPEAKER_DB_DSN for this project's specific postgres, not generic POSTGRES_DSN
            # The docker-compose maps 5433->5432 to avoid conflicts with local postgres
            pg_dsn = os.getenv("SPEAKER_DB_DSN") or "postgresql://diarization:diarization_dev@localhost:5433/speaker_embeddings"
            pg_client = PgVectorClient(pg_dsn)
            
            embedding_count = get_embedding_count()
            logger.info(f"   üìä PostgreSQL has {embedding_count} speaker embeddings")
            
            id_cache = IdentificationCache(
                video_id=video_id,
                strategy="knn",
                threshold=config.threshold,
                embedding_count=embedding_count,
            )
            
            # Log cache key and location for debugging
            logger.info(f"   üìÅ Cache key: {id_cache.cache_key}")
            logger.info(f"   üìÅ Cache file: {id_cache.cache_path}")
            
            # Convert dict segments to DiarizationSegment objects for identification
            from ingestion.instant_client import DiarizationSegment
            segment_objects = [DiarizationSegment.from_dict(s) for s in segments]
            
            if id_cache.has_range(cache_end_time):
                logger.info(f"   ‚úÖ Cache HIT: identification [0-{cache_end_time}s]")
                logger.info(f"   ‚Ü≥ Loading from: {id_cache.cache_path.name}")
                # Note: For identification, we'd need to reconstruct the plan
                # For now, we'll recompute (identification is fast)
                logger.info(f"   ‚Ü≥ Note: recomputing anyway (identification is fast)")
                identification_plan = identify_speakers(
                    instant_client=None,  # Not needed when segments provided
                    pg_client=pg_client,
                    video_id=video_id,
                    start_time=config.start_time if config.start_time > 0 else None,
                    end_time=config.end_time,
                    threshold=config.threshold,
                    audio_path=str(audio_path),
                    segments=segment_objects,  # Pass in-memory segments
                )
            else:
                cached_end = id_cache.get_cached_end()
                if cached_end:
                    logger.info(f"   ‚ö†Ô∏è Cache MISS: have [0-{cached_end}s], need [0-{cache_end_time}s]")
                else:
                    logger.info(f"   ‚ö†Ô∏è Cache MISS: no cache file found")
                logger.info(f"   ‚Ü≥ Will compute identification [0-{cache_end_time}s]")
                logger.info(f"   üîÑ Running KNN speaker identification...")
                identification_plan = identify_speakers(
                    instant_client=None,  # Not needed when segments provided
                    pg_client=pg_client,
                    video_id=video_id,
                    start_time=config.start_time if config.start_time > 0 else None,
                    end_time=config.end_time,
                    threshold=config.threshold,
                    audio_path=str(audio_path),
                    segments=segment_objects,  # Pass in-memory segments
                )
            
            logger.info(f"   üìä Identification result: {identification_plan.identified_count} identified, {identification_plan.unknown_count} unknown")
            
            # Log identified speakers
            if identification_plan.speaker_assignments:
                speakers = set(a.speaker_name for a in identification_plan.speaker_assignments if a.speaker_name)
                if speakers:
                    logger.info(f"   ‚Ü≥ Identified speakers: {', '.join(sorted(speakers))}")
            
        except Exception as e:
            logger.warning(f"Speaker identification failed: {e}")
    else:
        logger.info("Step 4: Skipping speaker identification (--skip-identify)")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PREVIEW: Generate and save markdown
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    logger.info("Step 5: Generating preview")
    
    # Print summary to console
    print_preview_summary(
        video_data=video_data,
        transcription_result=transcription_result,
        diarization_segments=segments,
        identification_plan=identification_plan,
        config=config,
    )
    
    # Generate full markdown with metrics
    preview_markdown = generate_preview_markdown(
        video_data=video_data,
        transcription_result=transcription_result,
        diarization_segments=segments,
        identification_plan=identification_plan,
        config=config,
        transcription_metrics=transcription_metrics.to_dict() if hasattr(transcription_metrics, 'to_dict') else None,
        diarization_metrics=diarization_metrics.to_dict() if hasattr(diarization_metrics, 'to_dict') else None,
    )
    
    # Save markdown file
    preview_path = save_preview(preview_markdown, video_id, also_print=False)
    
    # Preview mode = stop here, don't save
    if config.preview:
        print(f"\nüí° Preview mode: Nothing was saved to InstantDB.")
        print(f"   Preview file: {preview_path}")
        print(f"\n   To save, run with --yes:")
        print(f"   uv run audio_ingestion.py ingest \"{config.source}\" --start-time {config.start_time} --end-time {config.end_time or 'full'} --yes")
        return
        
    # Ask for confirmation (unless --yes is passed)
    if not config.yes:
        try:
            response = input("\nüíæ Save to InstantDB? [y/N] ")
            if response.lower() != 'y':
                print("‚ùå Cancelled. Nothing was saved.")
                print(f"   Preview file: {preview_path}")
                return
        except EOFError:
            print("‚ùå Non-interactive mode. Use --yes to auto-confirm.")
            return
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # SAVE: Actually write to InstantDB
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    logger.info("Step 6: Saving to InstantDB...")
    
    if instant_client is None:
        from ingestion.instant_client import InstantClient
        instant_client = InstantClient()
    
    try:
        # Prepare metrics
        trans_metrics = transcription_metrics.to_dict() if hasattr(transcription_metrics, 'to_dict') else {}
        diar_metrics = diarization_metrics.to_dict() if hasattr(diarization_metrics, 'to_dict') else {}
        
        # Save video + runs with metrics in one call
        # video_id here is the source ID (e.g., YouTube ID), InstantDB generates its own UUIDs
        # Use the actual workflow name (whisper_identified for synthetic segments)
        actual_workflow = "whisper_identified" if config.segment_source == "whisper" else config.workflow
        
        result = instant_client.save_ingestion_runs(
            source_id=video_id,  # e.g., "jAlKYYr1bpY" - stored as attribute
            video_title=config.title or audio_path.stem,
            video_filepath=str(audio_path.resolve()),
            video_url=config.source if config.is_url else None,
            video_duration=config.end_time or 0,
            transcription_metrics=trans_metrics,
            diarization_workflow=actual_workflow,
            diarization_metrics=diar_metrics,
            num_speakers_detected=len(set(s.get("speaker", "UNKNOWN") for s in segments)),
        )
        
        logger.info(f"   ‚úÖ Video saved: {result.get('video_id', 'N/A')[:8]}... (source: {video_id})")
        
        # Save words (individual entities for Ground Truth UI)
        if result.get("transcription_run_id") and transcription_result:
            trans_run_id = result["transcription_run_id"]
            
            # Flatten words from all segments
            words_to_save = []
            for seg_idx, segment in enumerate(transcription_result.segments):
                seg_words = segment.words if hasattr(segment, 'words') else segment.get('words', [])
                for word in seg_words:
                    # Handle both object and dict formats
                    if hasattr(word, 'word'):
                        text = word.word.strip()
                        start = word.start
                        end = word.end
                        confidence = getattr(word, 'probability', None)
                    else:
                        text = word.get('word', '').strip()
                        start = word.get('start')
                        end = word.get('end')
                        confidence = word.get('probability')
                    
                    # Skip words without valid timing
                    if start is None or end is None:
                        continue
                    
                    words_to_save.append({
                        "text": text,
                        "start_time": float(start),
                        "end_time": float(end),
                        "confidence": float(confidence) if confidence is not None else None,
                        "transcription_segment_index": seg_idx,
                    })
            
            if words_to_save:
                words_result = instant_client.save_words(trans_run_id, words_to_save)
                logger.info(f"   ‚úÖ Words saved: {words_result.get('count', 0)} words")
            
            logger.info(f"   ‚úÖ Transcription run: {trans_run_id[:8]}... (metrics: {trans_metrics.get('processing_time_seconds', 0):.2f}s, {trans_metrics.get('peak_memory_mb', 0):.1f}MB)")
        
        # Save diarization segments (individual entities for Ground Truth UI)
        if result.get("diarization_run_id") and segments:
            diar_run_id = result["diarization_run_id"]
            
            segments_to_save = []
            for seg in segments:
                segments_to_save.append({
                    "start_time": seg.get("start"),
                    "end_time": seg.get("end"),
                    "speaker_label": seg.get("speaker", "UNKNOWN"),
                    "confidence": seg.get("confidence"),
                })
            
            if segments_to_save:
                segs_result = instant_client.save_diarization_segments(diar_run_id, segments_to_save)
                logger.info(f"   ‚úÖ Segments saved: {segs_result.get('count', 0)} segments")
                
                # Get the saved segment IDs from the response
                saved_segment_ids = segs_result.get('segment_ids', [])
                
                # Save embeddings to PostgreSQL if segments have embeddings
                segments_with_embeddings = [
                    (seg, seg_id)
                    for seg, seg_id in zip(segments, saved_segment_ids)
                    if seg.get('embedding') is not None
                ]
                
                if segments_with_embeddings:
                    logger.info(f"   üîÑ Saving {len(segments_with_embeddings)} embeddings to PostgreSQL...")
                    
                    # Initialize PgVectorClient if not already done
                    if pg_client is None:
                        from src.embeddings.pgvector_client import PgVectorClient
                        pg_dsn = os.getenv("SPEAKER_DB_DSN") or "postgresql://diarization:diarization_dev@localhost:5433/speaker_embeddings"
                        pg_client = PgVectorClient(pg_dsn)
                    
                    # Get video_id from the result
                    video_uuid = result.get('video_id')
                    
                    embeddings_saved = 0
                    embedding_ids_to_update = []
                    
                    for seg, seg_id in segments_with_embeddings:
                        try:
                            # Save embedding to PostgreSQL
                            # Use speaker_id=None for unknown speakers (not "UNKNOWN" string)
                            pg_client.add_embedding(
                                external_id=seg_id,
                                embedding=seg['embedding'],
                                speaker_id=None,  # NULL for unknown speakers
                                speaker_label=seg.get('speaker', 'UNKNOWN'),
                                video_id=video_uuid,
                                diarization_run_id=diar_run_id,
                                start_time=seg.get('start'),
                                end_time=seg.get('end'),
                                metadata={
                                    "source": seg.get('source', 'whisper_transcription'),
                                    "transcription_segment_index": seg.get('transcription_segment_index'),
                                }
                            )
                            embeddings_saved += 1
                            
                            # Track segment_id -> embedding_id mapping for InstantDB update
                            # In our schema, external_id IS the segment_id, so embedding_id = segment_id
                            embedding_ids_to_update.append({
                                "segment_id": seg_id,
                                "embedding_id": seg_id,  # Same as segment_id since we use external_id
                            })
                        except Exception as e:
                            logger.warning(f"   ‚ö†Ô∏è Failed to save embedding for segment {seg_id[:8]}...: {e}")
                    
                    logger.info(f"   ‚úÖ Saved {embeddings_saved}/{len(segments_with_embeddings)} embeddings to PostgreSQL")
                    
                    # Update InstantDB segments with embedding_id
                    if embedding_ids_to_update:
                        try:
                            # Use the instant_client to update embedding_ids
                            update_result = instant_client.update_segment_embedding_ids(embedding_ids_to_update)
                            logger.info(f"   ‚úÖ Updated {update_result.get('count', 0)} segments with embedding_ids")
                        except Exception as e:
                            logger.warning(f"   ‚ö†Ô∏è Failed to update embedding_ids in InstantDB: {e}")
            
            logger.info(f"   ‚úÖ Diarization run: {diar_run_id[:8]}... (metrics: {diar_metrics.get('processing_time_seconds', 0):.2f}s, {diar_metrics.get('peak_memory_mb', 0):.1f}MB)")
        
    except Exception as e:
        logger.error(f"Failed to save to InstantDB: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Save identification results
    if identification_plan and not config.skip_identify and pg_client:
        try:
            from ingestion.identify import execute_plan
            execute_plan(instant_client, pg_client, identification_plan)
            logger.info(f"   ‚úÖ Identification saved: {identification_plan.identified_count} assignments")
        except Exception as e:
            logger.warning(f"Failed to save identification: {e}")
    
    # Summary
    total_time = time.time() - start_time_global
    print(f"""
{'‚ïê' * 60}
‚úÖ Ingestion Complete
{'‚ïê' * 60}
   Video ID: {video_id}
   Audio: {audio_path}
   Time range: {config.start_time}s - {config.end_time or 'end'}s
   Transcription: {len(transcription_result.segments)} segments
   Diarization: {len(segments)} segments
   Identification: {identification_plan.identified_count if identification_plan else 'skipped'}
   Total time: {total_time:.1f}s
   Preview: {preview_path}
{'‚ïê' * 60}
""")


def run_legacy_diarize(config: IngestionConfig) -> None:
    """
    Run legacy diarization workflow (backward compatibility).
    This handles the old --identify and --append-to flags.
    """
    # Check services  
    check_services(require_instant_server=True, require_postgres=True)
    
    logger.info(f"Processing clip: {config.clip_path}")
    
    if not config.clip_path.exists():
        logger.error(f"Clip not found: {config.clip_path}")
        return

    # Metadata collection
    git_info = get_git_info()
    start_time_global = time.time()
    
    # 1. Transcription
    logger.info("Starting transcription...")
    transcription_start = time.time()
    
    transcription_result = _get_or_create_transcription(config.clip_path)
        
    transcription_time = time.time() - transcription_start
    logger.info(f"Transcription complete in {transcription_time:.2f}s")

    # 2. Workflow Execution
    try:
        from ingestion.args import get_workflow
        workflow = get_workflow(config.workflow)
    except Exception as e:
        logger.error(f"Failed to initialize workflow: {e}")
        return

    segments, stats = workflow.run(config.clip_path, transcription_result)
    
    # Add transcription time to stats
    stats['transcription_time'] = transcription_time
    stats['total_time'] = time.time() - start_time_global
    
    # 3. Output Generation
    if not config.dry_run:
        from ingestion.report import generate_report
        generate_report(config, transcription_result.text, segments, stats, git_info)
        
        # Note: manifest.json is no longer supported
        # Data is now saved to InstantDB
        logger.info("Results saved. Note: manifest.json is deprecated, use InstantDB.")
    else:
        import tempfile
        logger.info("Dry run: Skipping output generation.")
        
        # Construct metadata
        metadata = {
            "command": f"python {' '.join(sys.argv)}",
            "cwd": os.getcwd(),
            "config": config.model_dump() if hasattr(config, 'model_dump') else config.dict()
        }
        
        output_data = {
            "metadata": metadata,
            "transcript": segments
        }
        
        # Print metadata to console (human-readable)
        print("\n--- Dry Run Metadata ---")
        print(f"Command: {metadata['command']}")
        print(f"CWD: {metadata['cwd']}")
        print(f"Workflow: {config.workflow.name}")
        print("------------------------\n")
        
        # Print transcript preview
        print(json.dumps(output_data, indent=2))
        
        # Save to temp file
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', prefix='dry_run_') as tmp:
            json.dump(output_data, tmp, indent=2)
            logger.info(f"Dry run results saved to temp file: {tmp.name}")
            print(f"\nDry run results saved to: {tmp.name}")


def _print_save_preview(
    video_data: dict,
    transcription_result: TranscriptionResult,
    diarization_segments: list,
    identification_plan,
    config: IngestConfig,
) -> None:
    """Print a preview of exactly what will be saved to InstantDB."""
    print("\n" + "‚ïê" * 72)
    print("üìã PREVIEW: What will be saved to InstantDB")
    print("‚ïê" * 72)
    
    # Video entity
    print(f"""
‚îå‚îÄ Video ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  id: {video_data['id']}
‚îÇ  title: {video_data['title']}
‚îÇ  filepath: {video_data['filepath'][:50]}...
‚îÇ  source_url: {video_data.get('source_url') or '(none)'}
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")
    
    # Transcription summary
    word_count = sum(len(seg.words) for seg in transcription_result.segments)
    print(f"""
‚îå‚îÄ Transcription ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Segments: {len(transcription_result.segments)}
‚îÇ  Words: {word_count}
‚îÇ  Language: {transcription_result.language}
‚îÇ  
‚îÇ  Sample (first 3 segments):""")
    
    for seg in transcription_result.segments[:3]:
        text_preview = seg.text[:60] + "..." if len(seg.text) > 60 else seg.text
        print(f"‚îÇ    [{seg.start:.1f}s-{seg.end:.1f}s] {text_preview}")
    
    if len(transcription_result.segments) > 3:
        print(f"‚îÇ    ... and {len(transcription_result.segments) - 3} more segments")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n")
    
    # Diarization summary
    speaker_labels = set(seg.get('speaker', 'UNKNOWN') for seg in diarization_segments)
    print(f"""
‚îå‚îÄ Diarization ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Segments: {len(diarization_segments)}
‚îÇ  Unique speaker labels: {len(speaker_labels)} ({', '.join(sorted(speaker_labels)[:5])})
‚îÇ  
‚îÇ  Sample (first 5 segments):""")
    
    for seg in diarization_segments[:5]:
        print(f"‚îÇ    [{seg.get('start', 0):.1f}s-{seg.get('end', 0):.1f}s] {seg.get('speaker', 'UNKNOWN')}: {seg.get('text', '')[:40]}...")
    
    if len(diarization_segments) > 5:
        print(f"‚îÇ    ... and {len(diarization_segments) - 5} more segments")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n")
    
    # Identification summary (if run)
    if identification_plan:
        from collections import Counter
        speaker_counts = Counter(
            r.identified_speaker for r in identification_plan.results 
            if r.status == "identified" and r.identified_speaker
        )
        
        print(f"""
‚îå‚îÄ Speaker Identification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Total processed: {len(identification_plan.results)}
‚îÇ  ‚úÖ Identified: {identification_plan.identified_count}
‚îÇ  ‚ùì Unknown: {identification_plan.unknown_count}
‚îÇ  ‚è≠Ô∏è  Skipped: {identification_plan.skipped_count}
‚îÇ  
‚îÇ  Identifications by speaker:""")
        
        for speaker, count in speaker_counts.most_common(5):
            print(f"‚îÇ    ‚Ä¢ {speaker}: {count} segments")
        
        if len(speaker_counts) > 5:
            print(f"‚îÇ    ... and {len(speaker_counts) - 5} more speakers")
        print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n")
    elif config.skip_identify:
        print("""
‚îå‚îÄ Speaker Identification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚è≠Ô∏è  SKIPPED (--skip-identify flag)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
""")
    
    print("‚ïê" * 72)


def _dict_to_transcription_result(data: dict) -> TranscriptionResult:
    """
    Convert a cached dict back to a TranscriptionResult object.
    
    Args:
        data: Dict from cache with text, segments, language
        
    Returns:
        TranscriptionResult object
    """
    segments = []
    for s in data.get('segments', []):
        words = [Word(**w) for w in s.get('words', [])]
        segments.append(Segment(
            start=s['start'],
            end=s['end'],
            text=s['text'],
            words=words,
            speaker=s.get('speaker', 'UNKNOWN')
        ))
    
    return TranscriptionResult(
        text=data.get('text', ''),
        segments=segments,
        language=data.get('language', 'en')
    )


def _get_or_create_transcription(
    audio_path: Path,
    start_time: float = 0,
    end_time: float = None,
) -> TranscriptionResult:
    """
    Get transcription from cache or create new one.
    
    Args:
        audio_path: Path to audio file
        start_time: Start time in seconds
        end_time: End time in seconds (None = full file)
        
    Returns:
        TranscriptionResult object
    """
    # Caching logic
    cache_dir = Path(__file__).parent / "data/cache/transcriptions"
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # Include time range in cache key if specified
    cache_key = audio_path.stem
    if start_time > 0 or end_time is not None:
        cache_key = f"{audio_path.stem}_{int(start_time)}_{int(end_time) if end_time else 'end'}"
    cache_file = cache_dir / f"{cache_key}.json"
    
    if cache_file.exists():
        logger.info(f"Loading transcription from cache: {cache_file}")
        try:
            with open(cache_file, 'r') as f:
                data = json.load(f)
                # Reconstruct Pydantic models
                segments = []
                for s in data['segments']:
                    words = [Word(**w) for w in s.get('words', [])]
                    segments.append(Segment(
                        start=s['start'],
                        end=s['end'],
                        text=s['text'],
                        words=words,
                        speaker=s.get('speaker', 'UNKNOWN')
                    ))
                return TranscriptionResult(
                    text=data['text'],
                    segments=segments,
                    language=data.get('language', 'en')
                )
        except Exception as e:
            logger.warning(f"Failed to load cache: {e}. Re-running transcription.")
    
    # Run transcription
    try:
        result = transcribe(
            str(audio_path),
            start_time=start_time if start_time > 0 else None,
            end_time=end_time,
        )
        
        # Save to cache
        with open(cache_file, 'w') as f:
            data = result.model_dump() if hasattr(result, 'model_dump') else result.dict()
            json.dump(data, f, indent=2)
        logger.info(f"Saved transcription to cache: {cache_file}")
        
        return result
    except Exception as e:
        logger.error(f"Transcription failed: {e}")
        raise


def _create_synthetic_diarization_segments(
    transcription_result: TranscriptionResult,
    audio_path: Path = None,
) -> tuple[list[dict], dict]:
    """
    Create synthetic diarization segments from Whisper transcription segments.
    
    This is useful when Whisper's natural segmentation (based on pauses and
    punctuation) produces better speaker boundaries than PyAnnote diarization.
    
    Each transcription segment becomes a diarization segment with:
    - speaker_label = "UNKNOWN" (to be identified later)
    - start_time, end_time from the transcription segment
    - text from the transcription segment
    - embedding: 512-dim voice embedding extracted from the audio segment
    
    Args:
        transcription_result: TranscriptionResult from Whisper
        audio_path: Path to the audio file for embedding extraction
        
    Returns:
        Tuple of (segments, stats) where:
        - segments: List of diarization segment dicts (with embeddings if audio_path provided)
        - stats: Dict with statistics about the synthetic segments
    """
    segments = []
    
    # Initialize embedding extractor if audio_path is provided
    extractor = None
    if audio_path is not None:
        try:
            from src.embeddings.pyannote_extractor import PyAnnoteEmbeddingExtractor
            extractor = PyAnnoteEmbeddingExtractor()
            logger.info(f"   üé§ Initialized PyAnnote embedding extractor")
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è Failed to initialize embedding extractor: {e}")
    
    total_segments = len(transcription_result.segments)
    embeddings_extracted = 0
    embeddings_failed = 0
    
    for idx, seg in enumerate(transcription_result.segments):
        # Get segment timing
        start = seg.start if hasattr(seg, 'start') else seg.get('start', 0)
        end = seg.end if hasattr(seg, 'end') else seg.get('end', 0)
        text = seg.text if hasattr(seg, 'text') else seg.get('text', '')
        
        # Extract embedding if extractor is available
        embedding = None
        if extractor is not None and audio_path is not None:
            logger.info(f"   üîä Extracting embedding {idx + 1}/{total_segments}...")
            try:
                embedding_np = extractor.extract_embedding(
                    audio_path=str(audio_path),
                    start_time=float(start),
                    end_time=float(end),
                )
                if embedding_np is not None:
                    embedding = embedding_np.tolist()
                    embeddings_extracted += 1
                else:
                    embeddings_failed += 1
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Failed to extract embedding for segment {idx}: {e}")
                embeddings_failed += 1
        
        # Create synthetic diarization segment
        segments.append({
            "start": float(start),
            "end": float(end),
            "speaker": "UNKNOWN",  # All segments start as UNKNOWN
            "text": text.strip(),
            "confidence": None,  # No confidence for synthetic segments
            "source": "whisper_transcription",  # Mark the source
            "transcription_segment_index": idx,
            "embedding": embedding,  # 512-dim embedding or None
        })
    
    # Calculate stats
    total_duration = sum(s["end"] - s["start"] for s in segments)
    avg_duration = total_duration / len(segments) if segments else 0
    
    stats = {
        "segment_count": len(segments),
        "total_duration": total_duration,
        "avg_segment_duration": avg_duration,
        "source": "whisper_transcription",
        "workflow": "whisper_identified",
        "embeddings_extracted": embeddings_extracted,
        "embeddings_failed": embeddings_failed,
    }
    
    if extractor is not None:
        logger.info(f"   ‚úÖ Extracted {embeddings_extracted}/{total_segments} embeddings ({embeddings_failed} failed)")
    
    return segments, stats


def main():
    config = parse_args()
    
    if hasattr(config, 'verbose') and config.verbose:
        logger.setLevel(logging.DEBUG)
    
    # Route to appropriate handler
    if isinstance(config, ServerConfig):
        run_server(config)
    elif isinstance(config, DownloadConfig):
        download_video(config)
    elif isinstance(config, TranscribeConfig):
        run_transcribe(config)
    elif isinstance(config, DiarizeConfig):
        run_diarize(config)
    elif isinstance(config, IdentifyConfig):
        run_identify(config)
    elif isinstance(config, IngestConfig):
        run_ingest(config)
    elif isinstance(config, IngestionConfig):
        # Legacy diarize command
        run_legacy_diarize(config)
    else:
        logger.error(f"Unknown config type: {type(config)}")


if __name__ == "__main__":
    main()
