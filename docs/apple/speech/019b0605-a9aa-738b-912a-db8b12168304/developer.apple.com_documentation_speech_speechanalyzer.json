{
  "markdown": "[Skip Navigation](https://developer.apple.com/documentation/speech/speechanalyzer#app-main)\n\n- [Global Nav Open Menu](https://developer.apple.com/documentation/speech/speechanalyzer#ac-gn-menustate) [Global Nav Close Menu](https://developer.apple.com/documentation/speech/speechanalyzer#)\n- [Apple Developer](https://developer.apple.com/)\n\n[Search Developer\\\\\n\\\\\nCancel](https://developer.apple.com/search/)\n\n- [Apple Developer](https://developer.apple.com/)\n- [News](https://developer.apple.com/news/)\n- [Discover](https://developer.apple.com/discover/)\n- [Design](https://developer.apple.com/design/)\n- [Develop](https://developer.apple.com/develop/)\n- [Distribute](https://developer.apple.com/distribute/)\n- [Support](https://developer.apple.com/support/)\n- [Account](https://developer.apple.com/account/)\n- [Search Developer](https://developer.apple.com/search/)\n\nCancel\n\nOnly search within “Documentation”\n\n### Quick Links\n\n- [Downloads](https://developer.apple.com/download/)\n- [Documentation](https://developer.apple.com/documentation/)\n- [Sample Code](https://developer.apple.com/documentation/samplecode/)\n- [Videos](https://developer.apple.com/videos/)\n- [Forums](https://developer.apple.com/forums/)\n\n5 Quick Links\n\n[Documentation](https://developer.apple.com/documentation)\n\n[Open Menu](https://developer.apple.com/documentation/speech/speechanalyzer#)\n\n- SwiftLanguage: Swift\n\n\nAll Technologies\n\n[**Speech**](https://developer.apple.com/documentation/speech)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n1 of 23 symbols inside <root>\n\n### Essentials\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n2 of 23 symbols inside <root> [Bringing advanced speech-to-text capabilities to your app](https://developer.apple.com/documentation/speech/bringing-advanced-speech-to-text-capabilities-to-your-app)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n3 of 23 symbols inside <root> containing 34 symbols [SpeechAnalyzer](https://developer.apple.com/documentation/speech/speechanalyzer)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n1 of 34 symbols inside -1414871264\n\n### Creating an analyzer\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n2 of 34 symbols inside -1414871264 [convenience init(modules: \\[any SpeechModule\\], options: SpeechAnalyzer.Options?)](https://developer.apple.com/documentation/speech/speechanalyzer/init(modules:options:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n3 of 34 symbols inside -1414871264 [convenience init<InputSequence>(inputSequence: InputSequence, modules: \\[any SpeechModule\\], options: SpeechAnalyzer.Options?, analysisContext: AnalysisContext, volatileRangeChangedHandler: sending ((CMTimeRange, Bool, Bool) -> Void)?)](https://developer.apple.com/documentation/speech/speechanalyzer/init(inputsequence:modules:options:analysiscontext:volatilerangechangedhandler:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n4 of 34 symbols inside -1414871264 [convenience init(inputAudioFile: AVAudioFile, modules: \\[any SpeechModule\\], options: SpeechAnalyzer.Options?, analysisContext: AnalysisContext, finishAfterFile: Bool, volatileRangeChangedHandler: sending ((CMTimeRange, Bool, Bool) -> Void)?) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/init(inputaudiofile:modules:options:analysiscontext:finishafterfile:volatilerangechangedhandler:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nS\n\n5 of 34 symbols inside -1414871264 containing 6 symbols [SpeechAnalyzer.Options](https://developer.apple.com/documentation/speech/speechanalyzer/options)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n6 of 34 symbols inside -1414871264\n\n### Managing modules\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n7 of 34 symbols inside -1414871264 [func setModules(\\[any SpeechModule\\]) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/setmodules(_:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nP\n\n8 of 34 symbols inside -1414871264 [var modules: \\[any SpeechModule\\]](https://developer.apple.com/documentation/speech/speechanalyzer/modules)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n9 of 34 symbols inside -1414871264\n\n### Performing analysis\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n10 of 34 symbols inside -1414871264 [func analyzeSequence<InputSequence>(InputSequence) async throws -> CMTime?](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(_:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n11 of 34 symbols inside -1414871264 [func analyzeSequence(from: AVAudioFile) async throws -> CMTime?](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(from:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n12 of 34 symbols inside -1414871264\n\n### Performing autonomous analysis\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n13 of 34 symbols inside -1414871264 [func start<InputSequence>(inputSequence: InputSequence) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputsequence:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n14 of 34 symbols inside -1414871264 [func start(inputAudioFile: AVAudioFile, finishAfterFile: Bool) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputaudiofile:finishafterfile:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n15 of 34 symbols inside -1414871264\n\n### Finalizing and cancelling results\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n16 of 34 symbols inside -1414871264 [func cancelAnalysis(before: CMTime)](https://developer.apple.com/documentation/speech/speechanalyzer/cancelanalysis(before:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n17 of 34 symbols inside -1414871264 [func finalize(through: CMTime?) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/finalize(through:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n18 of 34 symbols inside -1414871264\n\n### Finishing analysis\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n19 of 34 symbols inside -1414871264 [func cancelAndFinishNow() async](https://developer.apple.com/documentation/speech/speechanalyzer/cancelandfinishnow())\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n20 of 34 symbols inside -1414871264 [func finalizeAndFinishThroughEndOfInput() async throws](https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinishthroughendofinput())\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n21 of 34 symbols inside -1414871264 [func finalizeAndFinish(through: CMTime) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinish(through:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n22 of 34 symbols inside -1414871264 [func finish(after: CMTime) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/finish(after:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n23 of 34 symbols inside -1414871264\n\n### Determining audio formats\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n24 of 34 symbols inside -1414871264 [static func bestAvailableAudioFormat(compatibleWith: \\[any SpeechModule\\]) async -> AVAudioFormat?](https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n25 of 34 symbols inside -1414871264 [static func bestAvailableAudioFormat(compatibleWith: \\[any SpeechModule\\], considering: AVAudioFormat?) async -> AVAudioFormat?](https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:considering:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n26 of 34 symbols inside -1414871264\n\n### Improving responsiveness\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n27 of 34 symbols inside -1414871264 [func prepareToAnalyze(in: AVAudioFormat?) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n28 of 34 symbols inside -1414871264 [func prepareToAnalyze(in: AVAudioFormat?, withProgressReadyHandler: sending ((Progress) -> Void)?) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:withprogressreadyhandler:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n29 of 34 symbols inside -1414871264\n\n### Monitoring analysis\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n30 of 34 symbols inside -1414871264 [func setVolatileRangeChangedHandler(sending ((CMTimeRange, Bool, Bool) -> Void)?)](https://developer.apple.com/documentation/speech/speechanalyzer/setvolatilerangechangedhandler(_:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nP\n\n31 of 34 symbols inside -1414871264 [var volatileRange: CMTimeRange?](https://developer.apple.com/documentation/speech/speechanalyzer/volatilerange)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n32 of 34 symbols inside -1414871264\n\n### Managing contexts\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nM\n\n33 of 34 symbols inside -1414871264 [func setContext(AnalysisContext) async throws](https://developer.apple.com/documentation/speech/speechanalyzer/setcontext(_:))\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nP\n\n34 of 34 symbols inside -1414871264 [var context: AnalysisContext](https://developer.apple.com/documentation/speech/speechanalyzer/context)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n4 of 23 symbols inside <root> containing 11 symbols [AssetInventory](https://developer.apple.com/documentation/speech/assetinventory)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n5 of 23 symbols inside <root>\n\n### Modules\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n6 of 23 symbols inside <root> containing 16 symbols [SpeechTranscriber](https://developer.apple.com/documentation/speech/speechtranscriber)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n7 of 23 symbols inside <root> containing 15 symbols [DictationTranscriber](https://developer.apple.com/documentation/speech/dictationtranscriber)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n8 of 23 symbols inside <root> containing 7 symbols [SpeechDetector](https://developer.apple.com/documentation/speech/speechdetector)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nrP\n\n9 of 23 symbols inside <root> containing 6 symbols [SpeechModule](https://developer.apple.com/documentation/speech/speechmodule)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nrP\n\n10 of 23 symbols inside <root> containing 5 symbols [LocaleDependentSpeechModule](https://developer.apple.com/documentation/speech/localedependentspeechmodule)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n11 of 23 symbols inside <root>\n\n### Input and output\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nS\n\n12 of 23 symbols inside <root> containing 6 symbols [AnalyzerInput](https://developer.apple.com/documentation/speech/analyzerinput)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nrP\n\n13 of 23 symbols inside <root> containing 5 symbols [SpeechModuleResult](https://developer.apple.com/documentation/speech/speechmoduleresult)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n14 of 23 symbols inside <root>\n\n### Custom vocabulary\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n15 of 23 symbols inside <root> containing 8 symbols [AnalysisContext](https://developer.apple.com/documentation/speech/analysiscontext)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n16 of 23 symbols inside <root> containing 7 symbols [SFSpeechLanguageModel](https://developer.apple.com/documentation/speech/sfspeechlanguagemodel)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n17 of 23 symbols inside <root> containing 8 symbols [SFSpeechLanguageModel.Configuration](https://developer.apple.com/documentation/speech/sfspeechlanguagemodel/configuration)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n18 of 23 symbols inside <root> containing 28 symbols [SFCustomLanguageModelData](https://developer.apple.com/documentation/speech/sfcustomlanguagemodeldata)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n19 of 23 symbols inside <root>\n\n### Asset and resource management\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nC\n\n20 of 23 symbols inside <root> containing 2 symbols [AssetInstallationRequest](https://developer.apple.com/documentation/speech/assetinstallationrequest)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nE\n\n21 of 23 symbols inside <root> containing 2 symbols [SpeechModels](https://developer.apple.com/documentation/speech/speechmodels)\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\n22 of 23 symbols inside <root>\n\n### Legacy API\n\nTo navigate the symbols, press Up Arrow, Down Arrow, Left Arrow or Right Arrow\n\nCollection\n\n23 of 23 symbols inside <root> containing 27 symbols [Speech Recognition in Objective-C](https://developer.apple.com/documentation/speech/speech-recognition-in-objc)\n\n57 items were found. Tab back to navigate through them.\n\n/\n\nNavigator is ready\n\n- [Speech](https://developer.apple.com/documentation/speech)\n- SpeechAnalyzer\n\nClass\n\n# SpeechAnalyzer\n\nAnalyzes spoken audio content in various ways and manages the analysis session.\n\niOS 26.0+iPadOS 26.0+Mac Catalyst 26.0+macOS 26.0+visionOS 26.0+\n\n```\nfinal actor SpeechAnalyzer\n```\n\n## [Overview](https://developer.apple.com/documentation/speech/speechanalyzer\\#overview)\n\nThe Speech framework provides several modules that can be added to an analyzer to provide specific types of analysis and transcription. Many use cases only need a [`SpeechTranscriber`](https://developer.apple.com/documentation/speech/speechtranscriber) module, which performs speech-to-text transcriptions.\n\nThe `SpeechAnalyzer` class is responsible for:\n\n- Holding associated modules\n\n- Accepting audio speech input\n\n- Controlling the overall analysis\n\n\nEach module is responsible for:\n\n- Providing guidance on acceptable input\n\n- Providing its analysis or transcription output\n\n\nAnalysis is asynchronous. Input, output, and session control are decoupled and typically occur over several different tasks created by you or by the session. In particular, where an Objective-C API might use a delegate to provide results to you, the Swift API’s modules provides their results via an `AsyncSequence`. Similarly, you provide speech input to this API via an `AsyncSequence` you create and populate.\n\nThe analyzer can only analyze one input sequence at a time.\n\n### [Perform analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Perform-analysis)\n\nTo perform analysis on audio files and streams, follow these general steps:\n\n1. Create and configure the necessary modules.\n\n2. Ensure the relevant assets are installed or already present. See [`AssetInventory`](https://developer.apple.com/documentation/speech/assetinventory).\n\n3. Create an input sequence you can use to provide the spoken audio.\n\n4. Create and configure the analyzer with the modules and input sequence.\n\n5. Supply audio.\n\n6. Start analysis.\n\n7. Act on results.\n\n8. Finish analysis when desired.\n\n\nThis example shows how you could perform an analysis that transcribes audio using the `SpeechTranscriber` module:\n\n```\nimport Speech\n\n// Step 1: Modules\nguard let locale = SpeechTranscriber.supportedLocale(equivalentTo: Locale.current) else {\n    /* Note unsupported language */\n}\nlet transcriber = SpeechTranscriber(locale: locale, preset: .offlineTranscription)\n\n// Step 2: Assets\nif let installationRequest = try await AssetInventory.assetInstallationRequest(supporting: [transcriber]) {\n    try await installationRequest.downloadAndInstall()\n}\n\n// Step 3: Input sequence\nlet (inputSequence, inputBuilder) = AsyncStream.makeStream(of: AnalyzerInput.self)\n\n// Step 4: Analyzer\nlet audioFormat = await SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith: [transcriber])\nlet analyzer = SpeechAnalyzer(modules: [transcriber])\n\n// Step 5: Supply audio\nTask {\n    while /* audio remains */ {\n        /* Get some audio */\n        /* Convert to audioFormat */\n        let pcmBuffer = /* an AVAudioPCMBuffer containing some converted audio */\n        let input = AnalyzerInput(buffer: pcmBuffer)\n        inputBuilder.yield(input)\n    }\n    inputBuilder.finish()\n}\n\n// Step 7: Act on results\nTask {\n    do {\n        for try await result in transcriber.results {\n            let bestTranscription = result.text // an AttributedString\n            let plainTextBestTranscription = String(bestTranscription.characters) // a String\n            print(plainTextBestTranscription)\n        }\n    } catch {\n        /* Handle error */\n    }\n}\n\n// Step 6: Perform analysis\nlet lastSampleTime = try await analyzer.analyzeSequence(inputSequence)\n\n// Step 8: Finish analysis\nif let lastSampleTime {\n    try await analyzer.finalizeAndFinish(through: lastSampleTime)\n} else {\n    try analyzer.cancelAndFinishNow()\n}\n```\n\n### [Analyze audio files](https://developer.apple.com/documentation/speech/speechanalyzer\\#Analyze-audio-files)\n\nTo analyze one or more audio files represented by an `AVAudioFile` object, call methods such as [`analyzeSequence(from:)`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(from:)) or [`start(inputAudioFile:finishAfterFile:)`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputaudiofile:finishafterfile:)), or create the analyzer with one of the initializers that has a file parameter. These methods automatically convert the file to a supported audio format and process the file in its entirety.\n\nTo end the analysis session after one file, pass `true` for the `finishAfterFile` parameter or call one of the `finish` methods.\n\nOtherwise, by default, the analyzer won’t terminate its result streams and will wait for additional audio files or buffers. The analysis session doesn’t reset the audio timeline after each file; the next audio is assumed to come immediately after the completed file.\n\n### [Analyze audio buffers](https://developer.apple.com/documentation/speech/speechanalyzer\\#Analyze-audio-buffers)\n\nTo analyze audio buffers directly, convert them to a supported audio format, either on the fly or in advance. You can use [`bestAvailableAudioFormat(compatibleWith:)`](https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:)) or individual modules’ [`availableCompatibleAudioFormats`](https://developer.apple.com/documentation/speech/speechmodule/availablecompatibleaudioformats) methods to select a format to convert to.\n\nCreate an [`AnalyzerInput`](https://developer.apple.com/documentation/speech/analyzerinput) object for each audio buffer and add the object to an input sequence you create. Supply that input sequence to [`analyzeSequence(_:)`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(_:)), [`start(inputSequence:)`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputsequence:)), or a similar parameter of the analyzer’s initializer.\n\nTo skip past part of an audio stream, omit the buffers you want to skip from the input sequence. When you resume analysis with a later buffer, you can ensure the time-code of each module’s result accounts for the skipped audio. To do this, pass the later buffer’s time-code within the audio stream as the `bufferStartTime` parameter of the later `AnalyzerInput` object.\n\n### [Analyze autonomously](https://developer.apple.com/documentation/speech/speechanalyzer\\#Analyze-autonomously)\n\nYou can and usually should perform analysis using the [`analyzeSequence(_:)`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(_:)) or [`analyzeSequence(from:)`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(from:)) methods; those methods work well with Swift structured concurrency techniques. However, you may prefer that the analyzer proceed independently and perform its analysis autonomously as audio input becomes available in a task managed by the analyzer itself.\n\nTo use this capability, create the analyzer with one of the initializers that has an input sequence or file parameter, or call [`start(inputSequence:)`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputsequence:)) or [`start(inputAudioFile:finishAfterFile:)`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputaudiofile:finishafterfile:)). To end the analysis when the input ends, call [`finalizeAndFinishThroughEndOfInput()`](https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinishthroughendofinput()). To end the analysis of that input and start analysis of different input, call one of the `start` methods again.\n\n### [Control processing and timing of results](https://developer.apple.com/documentation/speech/speechanalyzer\\#Control-processing-and-timing-of-results)\n\nModules deliver results periodically, but you can manually synchronize their processing and delivery to outside cues.\n\nTo deliver a result for a particular time-code, call [`finalize(through:)`](https://developer.apple.com/documentation/speech/speechanalyzer/finalize(through:)). To cancel processing of results that are no longer of interest, call [`cancelAnalysis(before:)`](https://developer.apple.com/documentation/speech/speechanalyzer/cancelanalysis(before:)).\n\n### [Improve responsiveness](https://developer.apple.com/documentation/speech/speechanalyzer\\#Improve-responsiveness)\n\nBy default, the analyzer and modules load the system resources that they require lazily, and unload those resources when they’re deallocated.\n\nTo proactively load system resources and “preheat” the analyzer, call [`prepareToAnalyze(in:)`](https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:)) after setting its modules. This may improve how quickly the modules return their first results.\n\nTo delay or prevent unloading an analyzer’s resources — caching them for later use by a different analyzer instance — you can select a [`SpeechAnalyzer.Options.ModelRetention`](https://developer.apple.com/documentation/speech/speechanalyzer/options/modelretention-swift.enum) option and create the analyzer with an appropriate [`SpeechAnalyzer.Options`](https://developer.apple.com/documentation/speech/speechanalyzer/options) object.\n\nTo set the priority of analysis work, create the analyzer with a [`SpeechAnalyzer.Options`](https://developer.apple.com/documentation/speech/speechanalyzer/options) object given a `priority` value.\n\nSpecific modules may also offer options that improve responsiveness.\n\n### [Finish analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Finish-analysis)\n\nTo end an analysis session, you must use one of the analyzer’s `finish` methods or parameters, or deallocate the analyzer.\n\nWhen the analysis session transitions to the _finished_ state:\n\n- The analyzer won’t take additional input from the input sequence\n\n- Most methods won’t do anything; in particular, the analyzer won’t accept different input sequences or modules\n\n- Module result streams terminate and modules won’t publish additional results, though the app can continue to iterate over already-published results\n\n\nNote\n\nWhile you can terminate the input sequence you created with a method such as `AsyncStream.Continuation.finish()`, finishing the input sequence does _not_ cause the analysis session to become finished, and you can continue the session with a different input sequence.\n\n### [Respond to errors](https://developer.apple.com/documentation/speech/speechanalyzer\\#Respond-to-errors)\n\nWhen the analyzer or its modules’ result streams throw an error, the analysis session becomes finished as described above, and the same error (or a `CancellationError`) is thrown from all waiting methods and result streams.\n\n## [Topics](https://developer.apple.com/documentation/speech/speechanalyzer\\#topics)\n\n### [Creating an analyzer](https://developer.apple.com/documentation/speech/speechanalyzer\\#Creating-an-analyzer)\n\n[`convenience init(modules: [any SpeechModule], options: SpeechAnalyzer.Options?)`](https://developer.apple.com/documentation/speech/speechanalyzer/init(modules:options:))\n\nCreates an analyzer.\n\n[`convenience init<InputSequence>(inputSequence: InputSequence, modules: [any SpeechModule], options: SpeechAnalyzer.Options?, analysisContext: AnalysisContext, volatileRangeChangedHandler: sending ((CMTimeRange, Bool, Bool) -> Void)?)`](https://developer.apple.com/documentation/speech/speechanalyzer/init(inputsequence:modules:options:analysiscontext:volatilerangechangedhandler:))\n\nCreates an analyzer and begins analysis.\n\n[`convenience init(inputAudioFile: AVAudioFile, modules: [any SpeechModule], options: SpeechAnalyzer.Options?, analysisContext: AnalysisContext, finishAfterFile: Bool, volatileRangeChangedHandler: sending ((CMTimeRange, Bool, Bool) -> Void)?) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/init(inputaudiofile:modules:options:analysiscontext:finishafterfile:volatilerangechangedhandler:))\n\nCreates an analyzer and begins analysis on an audio file.\n\n[`struct Options`](https://developer.apple.com/documentation/speech/speechanalyzer/options)\n\nAnalysis processing options.\n\n### [Managing modules](https://developer.apple.com/documentation/speech/speechanalyzer\\#Managing-modules)\n\n[`func setModules([any SpeechModule]) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/setmodules(_:))\n\nAdds or removes modules.\n\n[`var modules: [any SpeechModule]`](https://developer.apple.com/documentation/speech/speechanalyzer/modules)\n\nThe modules performing analysis on the audio input.\n\n### [Performing analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Performing-analysis)\n\n[`func analyzeSequence<InputSequence>(InputSequence) async throws -> CMTime?`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(_:))\n\nAnalyzes an input sequence, returning when the sequence is consumed.\n\n[`func analyzeSequence(from: AVAudioFile) async throws -> CMTime?`](https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(from:))\n\nAnalyzes an input sequence created from an audio file, returning when the file has been read.\n\n### [Performing autonomous analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Performing-autonomous-analysis)\n\n[`func start<InputSequence>(inputSequence: InputSequence) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputsequence:))\n\nStarts analysis of an input sequence and returns immediately.\n\n[`func start(inputAudioFile: AVAudioFile, finishAfterFile: Bool) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/start(inputaudiofile:finishafterfile:))\n\nStarts analysis of an input sequence created from an audio file and returns immediately.\n\n### [Finalizing and cancelling results](https://developer.apple.com/documentation/speech/speechanalyzer\\#Finalizing-and-cancelling-results)\n\n[`func cancelAnalysis(before: CMTime)`](https://developer.apple.com/documentation/speech/speechanalyzer/cancelanalysis(before:))\n\nStops analyzing audio predating the given time.\n\n[`func finalize(through: CMTime?) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/finalize(through:))\n\nFinalizes the modules’ analyses.\n\n### [Finishing analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Finishing-analysis)\n\n[`func cancelAndFinishNow() async`](https://developer.apple.com/documentation/speech/speechanalyzer/cancelandfinishnow())\n\nFinishes analysis immediately.\n\n[`func finalizeAndFinishThroughEndOfInput() async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinishthroughendofinput())\n\nFinishes analysis after an audio input sequence has been fully consumed and its results are finalized.\n\n[`func finalizeAndFinish(through: CMTime) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinish(through:))\n\nFinishes analysis after finalizing results for a given time-code.\n\n[`func finish(after: CMTime) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/finish(after:))\n\nFinishes analysis once input for a given time is consumed.\n\n### [Determining audio formats](https://developer.apple.com/documentation/speech/speechanalyzer\\#Determining-audio-formats)\n\n[`static func bestAvailableAudioFormat(compatibleWith: [any SpeechModule]) async -> AVAudioFormat?`](https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:))\n\nRetrieves the best-quality audio format that the specified modules can work with, from assets installed on the device.\n\n[`static func bestAvailableAudioFormat(compatibleWith: [any SpeechModule], considering: AVAudioFormat?) async -> AVAudioFormat?`](https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:considering:))\n\nRetrieves the best-quality audio format that the specified modules can work with, taking into account the natural format of the audio and assets installed on the device.\n\n### [Improving responsiveness](https://developer.apple.com/documentation/speech/speechanalyzer\\#Improving-responsiveness)\n\n[`func prepareToAnalyze(in: AVAudioFormat?) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:))\n\nPrepares the analyzer to begin work with minimal startup delay.\n\n[`func prepareToAnalyze(in: AVAudioFormat?, withProgressReadyHandler: sending ((Progress) -> Void)?) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:withprogressreadyhandler:))\n\nPrepares the analyzer to begin work with minimal startup delay, reporting the progress of that preparation.\n\n### [Monitoring analysis](https://developer.apple.com/documentation/speech/speechanalyzer\\#Monitoring-analysis)\n\n[`func setVolatileRangeChangedHandler(sending ((CMTimeRange, Bool, Bool) -> Void)?)`](https://developer.apple.com/documentation/speech/speechanalyzer/setvolatilerangechangedhandler(_:))\n\nA closure that the analyzer calls when the volatile range changes.\n\n[`var volatileRange: CMTimeRange?`](https://developer.apple.com/documentation/speech/speechanalyzer/volatilerange)\n\nThe range of results that can change.\n\n### [Managing contexts](https://developer.apple.com/documentation/speech/speechanalyzer\\#Managing-contexts)\n\n[`func setContext(AnalysisContext) async throws`](https://developer.apple.com/documentation/speech/speechanalyzer/setcontext(_:))\n\nSets contextual information to improve or inform the analysis.\n\n[`var context: AnalysisContext`](https://developer.apple.com/documentation/speech/speechanalyzer/context)\n\nAn object containing contextual information.\n\n## [Relationships](https://developer.apple.com/documentation/speech/speechanalyzer\\#relationships)\n\n### [Conforms To](https://developer.apple.com/documentation/speech/speechanalyzer\\#conforms-to)\n\n- [`Actor`](https://developer.apple.com/documentation/Swift/Actor)\n- [`Sendable`](https://developer.apple.com/documentation/Swift/Sendable)\n- [`SendableMetatype`](https://developer.apple.com/documentation/Swift/SendableMetatype)\n\n## [See Also](https://developer.apple.com/documentation/speech/speechanalyzer\\#see-also)\n\n### [Essentials](https://developer.apple.com/documentation/speech/speechanalyzer\\#Essentials)\n\n[Bringing advanced speech-to-text capabilities to your app](https://developer.apple.com/documentation/speech/bringing-advanced-speech-to-text-capabilities-to-your-app)\n\nLearn how to incorporate live speech-to-text transcription into your app with SpeechAnalyzer.\n\n[`class AssetInventory`](https://developer.apple.com/documentation/speech/assetinventory)\n\nManages the assets that are necessary for transcription or other analyses.\n\nCurrent page is SpeechAnalyzer\n\n[Apple](https://www.apple.com/)\n\n1. [Developer](https://developer.apple.com/)\n2. [Documentation](https://developer.apple.com/documentation/)\n\n### Platforms\n\nToggle Menu\n\n- [iOS](https://developer.apple.com/ios/)\n- [iPadOS](https://developer.apple.com/ipados/)\n- [macOS](https://developer.apple.com/macos/)\n- [tvOS](https://developer.apple.com/tvos/)\n- [visionOS](https://developer.apple.com/visionos/)\n- [watchOS](https://developer.apple.com/watchos/)\n\n### Tools\n\nToggle Menu\n\n- [Swift](https://developer.apple.com/swift/)\n- [SwiftUI](https://developer.apple.com/swiftui/)\n- [Swift Playground](https://developer.apple.com/swift-playground/)\n- [TestFlight](https://developer.apple.com/testflight/)\n- [Xcode](https://developer.apple.com/xcode/)\n- [Xcode Cloud](https://developer.apple.com/xcode-cloud/)\n- [SF Symbols](https://developer.apple.com/sf-symbols/)\n\n### Topics & Technologies\n\nToggle Menu\n\n- [Accessibility](https://developer.apple.com/accessibility/)\n- [Accessories](https://developer.apple.com/accessories/)\n- [App Extension](https://developer.apple.com/app-extensions/)\n- [App Store](https://developer.apple.com/app-store/)\n- [Audio & Video](https://developer.apple.com/audio/)\n- [Augmented Reality](https://developer.apple.com/augmented-reality/)\n- [Design](https://developer.apple.com/design/)\n- [Distribution](https://developer.apple.com/distribute/)\n- [Education](https://developer.apple.com/education/)\n- [Fonts](https://developer.apple.com/fonts/)\n- [Games](https://developer.apple.com/games/)\n- [Health & Fitness](https://developer.apple.com/health-fitness/)\n- [In-App Purchase](https://developer.apple.com/in-app-purchase/)\n- [Localization](https://developer.apple.com/localization/)\n- [Maps & Location](https://developer.apple.com/maps/)\n- [Machine Learning & AI](https://developer.apple.com/machine-learning/)\n- [Open Source](https://opensource.apple.com/)\n- [Security](https://developer.apple.com/security/)\n- [Safari & Web](https://developer.apple.com/safari/)\n\n### Resources\n\nToggle Menu\n\n- [Documentation](https://developer.apple.com/documentation/)\n- [Tutorials](https://developer.apple.com/learn/)\n- [Downloads](https://developer.apple.com/download/)\n- [Forums](https://developer.apple.com/forums/)\n- [Videos](https://developer.apple.com/videos/)\n\n### Support\n\nToggle Menu\n\n- [Support Articles](https://developer.apple.com/support/articles/)\n- [Contact Us](https://developer.apple.com/contact/)\n- [Bug Reporting](https://developer.apple.com/bug-reporting/)\n- [System Status](https://developer.apple.com/system-status/)\n\n### Account\n\nToggle Menu\n\n- [Apple Developer](https://developer.apple.com/account/)\n- [App Store Connect](https://appstoreconnect.apple.com/)\n- [Certificates, IDs, & Profiles](https://developer.apple.com/account/ios/certificate/)\n- [Feedback Assistant](https://feedbackassistant.apple.com/)\n\n### Programs\n\nToggle Menu\n\n- [Apple Developer Program](https://developer.apple.com/programs/)\n- [Apple Developer Enterprise Program](https://developer.apple.com/programs/enterprise/)\n- [App Store Small Business Program](https://developer.apple.com/app-store/small-business-program/)\n- [MFi Program](https://mfi.apple.com/)\n- [News Partner Program](https://developer.apple.com/programs/news-partner/)\n- [Video Partner Program](https://developer.apple.com/programs/video-partner/)\n- [Security Bounty Program](https://developer.apple.com/security-bounty/)\n- [Security Research Device Program](https://developer.apple.com/programs/security-research-device/)\n\n### Events\n\nToggle Menu\n\n- [Meet with Apple](https://developer.apple.com/events/)\n- [Apple Developer Centers](https://developer.apple.com/events/developer-centers/)\n- [App Store Awards](https://developer.apple.com/app-store/app-store-awards/)\n- [Apple Design Awards](https://developer.apple.com/design/awards/)\n- [Apple Developer Academies](https://developer.apple.com/academies/)\n- [WWDC](https://developer.apple.com/wwdc/)\n\nTo submit feedback on documentation, visit [Feedback Assistant](applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fspeech%2Fspeechanalyzer).\n\nSelect a color scheme preference\nLight\n\nDark\n\nAuto\n\nCopyright © 2025 [Apple Inc.](https://www.apple.com/) All rights reserved.\n\n[Terms of Use](https://www.apple.com/legal/internet-services/terms/site.html) [Privacy Policy](https://www.apple.com/legal/privacy/) [Agreements and Guidelines](https://developer.apple.com/support/terms/)",
  "metadata": {
    "og:title": "SpeechAnalyzer | Apple Developer Documentation",
    "og:type": "website",
    "og:url": "https://developer.apple.com/documentation/speech/speechanalyzer",
    "twitter:description": "Analyzes spoken audio content in various ways and manages the analysis session.",
    "ogImage": "https://developer.apple.com/tutorials/developer-og.jpg",
    "ogTitle": "SpeechAnalyzer | Apple Developer Documentation",
    "language": "en-US",
    "twitter:card": "summary_large_image",
    "twitter:title": "SpeechAnalyzer | Apple Developer Documentation",
    "ogUrl": "https://developer.apple.com/documentation/speech/speechanalyzer",
    "twitter:url": "https://developer.apple.com/documentation/speech/speechanalyzer",
    "twitter:image": "https://developer.apple.com/tutorials/developer-og-twitter.jpg",
    "ogDescription": "Analyzes spoken audio content in various ways and manages the analysis session.",
    "title": "SpeechAnalyzer | Apple Developer Documentation",
    "og:description": "Analyzes spoken audio content in various ways and manages the analysis session.",
    "ogSiteName": "Apple Developer Documentation",
    "og:site_name": "Apple Developer Documentation",
    "viewport": "width=device-width,initial-scale=1,viewport-fit=cover",
    "description": "Analyzes spoken audio content in various ways and manages the analysis session.",
    "og:image": "https://developer.apple.com/tutorials/developer-og.jpg",
    "og:locale": "en",
    "ogLocale": "en",
    "favicon": "https://developer.apple.com/favicon.ico",
    "scrapeId": "019b0605-cbcf-7490-9bd1-00bf644f7af2",
    "sourceURL": "https://developer.apple.com/documentation/speech/speechanalyzer",
    "url": "https://developer.apple.com/documentation/speech/speechanalyzer",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "timezone": "America/New_York",
    "proxyUsed": "basic",
    "cacheState": "miss",
    "indexId": "6e74ac3a-d099-4c59-8372-58e6a21fc25b",
    "creditsUsed": 5
  },
  "links": [
    "https://developer.apple.com/documentation/speech/speechanalyzer#app-main",
    "https://developer.apple.com/documentation/speech/speechanalyzer#ac-gn-menustate",
    "https://developer.apple.com/documentation/speech/speechanalyzer#",
    "https://developer.apple.com/",
    "https://developer.apple.com/search/",
    "https://developer.apple.com/news/",
    "https://developer.apple.com/discover/",
    "https://developer.apple.com/design/",
    "https://developer.apple.com/develop/",
    "https://developer.apple.com/distribute/",
    "https://developer.apple.com/support/",
    "https://developer.apple.com/account/",
    "https://developer.apple.com/download/",
    "https://developer.apple.com/documentation/",
    "https://developer.apple.com/documentation/samplecode/",
    "https://developer.apple.com/videos/",
    "https://developer.apple.com/forums/",
    "https://developer.apple.com/documentation",
    "https://developer.apple.com/documentation/speech",
    "https://developer.apple.com/documentation/speech/bringing-advanced-speech-to-text-capabilities-to-your-app",
    "https://developer.apple.com/documentation/speech/speechanalyzer",
    "https://developer.apple.com/documentation/speech/speechanalyzer/init(modules:options:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/init(inputsequence:modules:options:analysiscontext:volatilerangechangedhandler:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/init(inputaudiofile:modules:options:analysiscontext:finishafterfile:volatilerangechangedhandler:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/options",
    "https://developer.apple.com/documentation/speech/speechanalyzer/setmodules(_:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/modules",
    "https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(_:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/analyzesequence(from:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/start(inputsequence:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/start(inputaudiofile:finishafterfile:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/cancelanalysis(before:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/finalize(through:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/cancelandfinishnow()",
    "https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinishthroughendofinput()",
    "https://developer.apple.com/documentation/speech/speechanalyzer/finalizeandfinish(through:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/finish(after:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/bestavailableaudioformat(compatiblewith:considering:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/preparetoanalyze(in:withprogressreadyhandler:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/setvolatilerangechangedhandler(_:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/volatilerange",
    "https://developer.apple.com/documentation/speech/speechanalyzer/setcontext(_:)",
    "https://developer.apple.com/documentation/speech/speechanalyzer/context",
    "https://developer.apple.com/documentation/speech/assetinventory",
    "https://developer.apple.com/documentation/speech/speechtranscriber",
    "https://developer.apple.com/documentation/speech/dictationtranscriber",
    "https://developer.apple.com/documentation/speech/speechdetector",
    "https://developer.apple.com/documentation/speech/speechmodule",
    "https://developer.apple.com/documentation/speech/localedependentspeechmodule",
    "https://developer.apple.com/documentation/speech/analyzerinput",
    "https://developer.apple.com/documentation/speech/speechmoduleresult",
    "https://developer.apple.com/documentation/speech/analysiscontext",
    "https://developer.apple.com/documentation/speech/sfspeechlanguagemodel",
    "https://developer.apple.com/documentation/speech/sfspeechlanguagemodel/configuration",
    "https://developer.apple.com/documentation/speech/sfcustomlanguagemodeldata",
    "https://developer.apple.com/documentation/speech/assetinstallationrequest",
    "https://developer.apple.com/documentation/speech/speechmodels",
    "https://developer.apple.com/documentation/speech/speech-recognition-in-objc",
    "https://developer.apple.com/documentation/speech/speechanalyzer#overview",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Perform-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Analyze-audio-files",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Analyze-audio-buffers",
    "https://developer.apple.com/documentation/speech/speechmodule/availablecompatibleaudioformats",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Analyze-autonomously",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Control-processing-and-timing-of-results",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Improve-responsiveness",
    "https://developer.apple.com/documentation/speech/speechanalyzer/options/modelretention-swift.enum",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Finish-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Respond-to-errors",
    "https://developer.apple.com/documentation/speech/speechanalyzer#topics",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Creating-an-analyzer",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Managing-modules",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Performing-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Performing-autonomous-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Finalizing-and-cancelling-results",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Finishing-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Determining-audio-formats",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Improving-responsiveness",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Monitoring-analysis",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Managing-contexts",
    "https://developer.apple.com/documentation/speech/speechanalyzer#relationships",
    "https://developer.apple.com/documentation/speech/speechanalyzer#conforms-to",
    "https://developer.apple.com/documentation/Swift/Actor",
    "https://developer.apple.com/documentation/Swift/Sendable",
    "https://developer.apple.com/documentation/Swift/SendableMetatype",
    "https://developer.apple.com/documentation/speech/speechanalyzer#see-also",
    "https://developer.apple.com/documentation/speech/speechanalyzer#Essentials",
    "https://www.apple.com/",
    "https://developer.apple.com/ios/",
    "https://developer.apple.com/ipados/",
    "https://developer.apple.com/macos/",
    "https://developer.apple.com/tvos/",
    "https://developer.apple.com/visionos/",
    "https://developer.apple.com/watchos/",
    "https://developer.apple.com/swift/",
    "https://developer.apple.com/swiftui/",
    "https://developer.apple.com/swift-playground/",
    "https://developer.apple.com/testflight/",
    "https://developer.apple.com/xcode/",
    "https://developer.apple.com/xcode-cloud/",
    "https://developer.apple.com/sf-symbols/",
    "https://developer.apple.com/accessibility/",
    "https://developer.apple.com/accessories/",
    "https://developer.apple.com/app-extensions/",
    "https://developer.apple.com/app-store/",
    "https://developer.apple.com/audio/",
    "https://developer.apple.com/augmented-reality/",
    "https://developer.apple.com/education/",
    "https://developer.apple.com/fonts/",
    "https://developer.apple.com/games/",
    "https://developer.apple.com/health-fitness/",
    "https://developer.apple.com/in-app-purchase/",
    "https://developer.apple.com/localization/",
    "https://developer.apple.com/maps/",
    "https://developer.apple.com/machine-learning/",
    "https://opensource.apple.com/",
    "https://developer.apple.com/security/",
    "https://developer.apple.com/safari/",
    "https://developer.apple.com/learn/",
    "https://developer.apple.com/support/articles/",
    "https://developer.apple.com/contact/",
    "https://developer.apple.com/bug-reporting/",
    "https://developer.apple.com/system-status/",
    "https://appstoreconnect.apple.com/",
    "https://developer.apple.com/account/ios/certificate/",
    "https://feedbackassistant.apple.com/",
    "https://developer.apple.com/programs/",
    "https://developer.apple.com/programs/enterprise/",
    "https://developer.apple.com/app-store/small-business-program/",
    "https://mfi.apple.com/",
    "https://developer.apple.com/programs/news-partner/",
    "https://developer.apple.com/programs/video-partner/",
    "https://developer.apple.com/security-bounty/",
    "https://developer.apple.com/programs/security-research-device/",
    "https://developer.apple.com/events/",
    "https://developer.apple.com/events/developer-centers/",
    "https://developer.apple.com/app-store/app-store-awards/",
    "https://developer.apple.com/design/awards/",
    "https://developer.apple.com/academies/",
    "https://developer.apple.com/wwdc/",
    "applefeedback://new?form_identifier=developertools.fba&answers%5B%3Aarea%5D=seedADC%3Adevpubs&answers%5B%3Adoc_type_req%5D=Technology%20Documentation&answers%5B%3Adocumentation_link_req%5D=https%3A%2F%2Fdeveloper.apple.com%2Fdocumentation%2Fspeech%2Fspeechanalyzer",
    "https://www.apple.com/legal/internet-services/terms/site.html",
    "https://www.apple.com/legal/privacy/",
    "https://developer.apple.com/support/terms/"
  ],
  "json": {
    "company_name": "Apple Inc.",
    "company_description": "Apple Inc. is a multinational technology company that designs, manufactures, and markets consumer electronics, computer hardware and software, and online services."
  },
  "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan."
}